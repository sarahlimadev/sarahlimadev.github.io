---
layout: post
image: assets/img/post_07_08/header_post_07_08.png
image_caption: Uma imagem ilustrativa do avanço dos Agentes de IA
title: "Do Zero ao Um: Aprendendo Modelos de Agente"
subtitle: Como construir sistemas agênticos robustos e eficazes?
date: 2025-07-08
author: Philipp Schmid
tradutor: Gabriel A. Belo
categories: [Agentes de IA]
tags: [IA Generativa, Chatbots]
---

Agentes de IA. IA Agêntica. Arquitetura Agêntica. *Workflow* Agênticos. Modelos Agênticos. Agentes estão em todos os lugares. Mas o que exatamente são eles, e como construir sistemas agênticos robustos e eficazes? Enquanto o termo "agente" é amplamente usado, uma característica fundamental é a sua capacidade de planejar e executar tarefas dinamicamente, frequentemente utilizando **ferramentas externas** e **memória** para alcançar objetivos complexos.

Este post tem como objetivo explorar modelos de design comuns. Visualizar essas estruturas como padrões reutilizáveis para construir aplicações de IA. Compreendê-los oferece um modelo mental para solucionar problemas complexos e projetar sistemas que sejam escaláveis, modulares e adaptáveis.

Vamos aprofundar em diversos padrões comuns diferenciando entre *workflows* mais estruturados e padrões agênticos mais dinâmicos. *Workflows* geralmente seguem caminhos predefinidos, enquanto agentes possuem mais autonomia para decidir seu curso de ação.

### Por que "Padrões Agênticos" importam?

- Padrões oferecem uma maneira estruturada de pensar e projetar sistemas.

- Padrões nos permitem construir e expandir aplicações de IA em complexidade e adaptar a requisitos variáveis. Designs modulares baseados em padrões são mais fáceis de modificar e estender.

- Padrões ajudam a gerenciar a complexidade da coordenação de múltiplos agentes, ferramentas e fluxos de trabalho, oferecendo modelos reutilizáveis e confiáveis. Eles promovem as melhores práticas e o entendimento compartilhado entre os desenvolvedores.

### Quando (e quando não) usar Agentes?

Antes de mergulharmos nos padrões, é crucial considerar quando uma abordagem agêntica é realmente necessária.

- Sempre busque a solução mais simples primeiro. Se você souber os passos exatos necessários para resolver um problema, um fluxo de trabalho fixo ou até mesmo um script simples pode ser mais eficiente e acertivo do que um agente.

- Sistemas agênticos frequentemente trocam o aumento da latência e do custo computacional por um desempenho potencialmente melhor em tarefas complexas, ambíguas ou dinâmicas. Certifique-se de que os benefícios superam esses custos.

- Use *workflows* para previsibilidade e consistência ao lidar com tarefas bem definidas onde os passos são conhecidos.

- Use agentes quando flexibilidade, adaptabilidade e tomadas de decisões orientadas por modelos são necessárias.

- Mantenha a Simplicidade (ainda que com agentes): Mesmo ao construir sistemas agênticos, esforce-se pelo design mais simples e eficaz. Agentes excessivamente complexos podem se tornar difíceis de depurar e gerenciar.

- Agentes introduzem imprevisibilidade inerente e potenciais erros. Sistemas agênticos devem incorporar registro de erros robusto, tratamento de exceções e mecanismos de repetição, dando ao sistema (ou ao LLM subjacente) a chance de se autocorrigir.

Abaixo, exploraremos 3 padrões de *workflows* comuns e 4 padrões agênticos. Ilustraremos cada um usando chamadas de API puras, sem depender de frameworks específicos como LangChain, LangGraph, LlamaIndex ou CrewAI, para focar nos conceitos centrais.

### Visão Geral dos Padrões

Vamos cobrir os seguintes modelos:

- [*Workflow*: Prompt encadeado](#workflow-prompt-encadeado)
- [*Workflow*: Roteamento ou *Handoff*](#workflow-roteamento-ou-handoff)
- [*Workflow*: Paralelização](#workflow-paralelização)
- [Padrão de Reflexão](#padrão-de-reflexão)
- [Padrão de Uso de Ferramentas](#padrão-de-uso-de-ferramentas)
- [Padrão: Planejamento (Orquestrador-Trabalhadores)](#padrão-planejamento-orquestrador-trabalhadores)
- [Padrão: Multiagente](#padrão-multiagente)

### *Workflow*: Prompt encadeado

![prompt-encadeado]({{ site.baseurl }}/assets/img/post_07_08/prompt-chaining.png)

A saída de uma chamada de *LLM* alimenta sequencialmente a entrada da próxima chamada de *LLM*. Esse padrão decompõe uma tarefa em uma sequência fixa de passos. Cada passo é tratado por uma chamada de *LLM* que processa a saída da anterior. É adequado para tarefas que podem ser divididas de forma clara em subtarefas previsíveis e sequenciais.

#### Casos de Uso

- **Gerar um documento estruturado**: *LLM 1* cria um esboço, *LLM 2* valida o esboço contra critérios, *LLM 3* escreve o conteúdo com base no esboço validado.

- **Processamento de dados em várias etapas**: Extrair informações, transformá-las e depois resumi-las.

- **Gerar conjunto de notícias com base em entradas curadas**.

```python

import os
from google import genai
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# --- Passo 1: Resuma o texto ---
original_text = "Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way."
prompt1 = f"Resuma o seguinte texto em uma setença: {original_text}"
 
# Use "client.models.generate_content"
response1 = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=prompt1
)
summary = response1.text.strip()
print(f"Resumo: {summary}")
 
# --- Passo 2: Traduza o sumário---
prompt2 = f"Traduza o seginte texto em Português, retorne apenas a tradução, sem texto adicional: {summary}"
 
# Use "client.models.generate_content"
response2 = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=prompt2
)
translation = response2.text.strip()
print(f"Tradução: {translation}")

```

### *Workflow*: Roteamento ou Handoff

![routeamento]({{ site.baseurl }}/assets/img/post_07_08/routing-or-handoff.png)

Um *LLM* inicial atua como um roteador, classificando a entrada do usuário e direcionando-a para a tarefa especializada ou *LLM* mais apropriada. Esse padrão implementa uma separação de responsabilidades e permite otimizar tarefas individuais (usando prompts especializados, modelos diferentes ou ferramentas específicas) isoladamente. Ele melhora a eficiência e potencialmente reduz custos ao usar modelos menores para tarefas mais simples. Quando uma tarefa é roteada, o agente selecionado "assume" a responsabilidade pela conclusão.

> O termo *"handoff"* é uma expressão americana que significa a passagem de responsabilidades de um funcionário ou departamento para outro mais qualificado.

#### Casos de Uso

- **Sistemas de suporte ao cliente**: Roteamento de consultas para agentes especializados em cobrança, suporte técnico ou informações de produtos.

- **Uso de LLM em camadas**: Roteamento de consultas simples para modelos mais rápidos e baratos (como Llama 3.1 8B) e perguntas complexas ou incomuns para modelos mais capazes (como Gemini 1.5 Pro).

- **Geração de conteúdo**: Roteamento de solicitações para postagens de blog, atualizações de mídias sociais ou textos de anúncios para diferentes prompts/modelos especializados.

```python

import os
import json
from google import genai
from pydantic import BaseModel
import enum
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# "Schema" de Roteamento
class Category(enum.Enum):
    WEATHER = "clima"
    SCIENCE = "ciência"
    UNKNOWN = "desconhecido"
 
class RoutingDecision(BaseModel):
    category: Category
    reasoning: str
 
# Passo 1: Routeie a requisição
user_query = "Qual é o clima em Recife?"
 
prompt_router = f"""
Analise a requisição do usuário abaixo e determine qual é a sua categoria.
Categorias:
- Clima: Para questões sobre condições climáticas.
- Ciência: Para questões sobre ciência.
- Desconhecido: Para uma categoria não especificada.
 
Requisição: {user_query}
"""
 
# Use "client.models.generate_content" com "config" para uma saída estruturada
response_router = client.models.generate_content(
    model= 'gemini-2.0-flash-lite',
    contents=prompt_router,
    config={
        'response_mime_type': 'application/json',
        'response_schema': RoutingDecision,
    },
)
print(f"Decisão de Roteamento: Categoria={response_router.parsed.category}, Raciocínio={response_router.parsed.reasoning}")
 
# Step 2: Encaminhamento baseado em Roteamento
final_response = ""
if response_router.parsed.category == Category.WEATHER:
    weather_prompt = f"Forneça uma breve previsão do tempo do local mencionado: '{user_query}'"
    weather_response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=weather_prompt
    )
    final_response = weather_response.text

elif response_router.parsed.category == Category.SCIENCE:
    science_response = client.models.generate_content(
        model="gemini-2.5-flash-preview-04-17",
        contents=user_query
    )
    final_response = science_response.text

else:
    unknown_response = client.models.generate_content(
        model="gemini-2.0-flash-lite",
        contents=f"A requisição do usuário foi essa: {prompt_router}, mas ela não pode ser respondida. Aqui está o raciocíonio: {response_router.parsed.reasoning}. Escreva um pedido para que o usuário tente novamente."
    )
    final_response = unknown_response.text

print(f"\nResposta Final: {final_response}")

```

### *Workflow*: Paralelização

![paralelização]({{ site.baseurl }}/assets/img/post_07_08/parallelization.png)

Uma tarefa é dividida em subtarefas independentes que são processadas simultaneamente por múltiplos *LLMs*, com suas saídas sendo agregadas. Esse padrão utiliza a concorrência para as tarefas. A consulta inicial (ou partes dela) é enviada para múltiplos *LLMs* em paralelo com prompts/objetivos individuais. Uma vez que todos os fluxos são concluídos, seus resultados individuais são coletados e passados para um *LLM* agregador final, que os sintetiza a resposta final. Isso pode melhorar a latência se as subtarefas não dependerem umas das outras, ou aumentar a qualidade por meio de técnicas como votação majoritária ou geração de opções diversas.

#### Casos de Uso

- **RAG com decomposição de consulta**: Quebrar uma consulta complexa em subconsultas, executar recuperações para cada uma em paralelo e sintetizar os resultados.

- **Análise de documentos grandes**: Dividir o documento em seções, resumir cada seção em paralelo e, em seguida, combinar os resumos.

- **Geração de múltiplas perspectivas**: Fazer a mesma pergunta a vários LLMs com diferentes prompts de persona e agregar suas respostas.

- **Operações de estilo Map-reduce em dados**.

```python

import os
import asyncio
import time
from google import genai
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
async def generate_content(prompt: str) -> str:
        response = await client.aio.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        return response.text.strip()
 
async def parallel_tasks():
    # Defina tarefas paralelas
    topic = "Um robô amigável explorando a Selva!"
    prompts = [
        f"Escreva uma pequena história de aventura sobre {topic}.",
        f"Escreva uma pequena história engraçada sobre {topic}.",
        f"Escreva uma pequena história de mistério sobre {topic}."
    ]
    # Execute as tarefas em cadeia e colete os resultados
    start_time = time.time()
    tasks = [generate_content(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)
    end_time = time.time()
    print(f"Tempo decorrido: {end_time - start_time} segundos")
 
    print("\n--- Resultados individuais ---")
    for i, result in enumerate(results):
        print(f"Resultado {i+1}: {result}\n")
 
    # Junte os resultados e gere a história final results and generate final story
    story_ideas = '\n'.join([f"Ideia {i+1}: {result}" for i, result in enumerate(results)])
    aggregation_prompt = f"Combine as três histórias a seguir em um único resumo coerente:{story_ideas}"
    aggregation_response = await client.aio.models.generate_content(
        model="gemini-2.5-flash-preview-04-17",
        contents=aggregation_prompt
    )
    return aggregation_response.text
    
 
result = await parallel_tasks()
print(f"\n--- Resumo Agregado ---\n{result}")

```

### Padrão de Reflexão

![reflexão]({{ site.baseurl }}/assets/img/post_07_08/reflection.png)

Um agente avalia sua própria saída e usa esse feedback para refinar sua resposta iterativamente. Este padrão também é conhecido como Avaliador-Otimizador e utiliza um loop de autocorreção. Um *LLM* inicial gera uma resposta ou completa uma tarefa. Uma segunda etapa do *LLM* (ou até mesmo o mesmo *LLM* com um prompt diferente) atua como um refletor ou avaliador, criticando a saída inicial em relação aos requisitos ou à qualidade desejada. Essa crítica (feedback) é então realimentada, impulsionando o *LLM* a produzir uma saída refinada. Esse ciclo pode se repetir até que o avaliador confirme que os requisitos foram atendidos ou que uma saída satisfatória foi alcançada.

#### Casos de Uso

- **Geração de código**: Escrever código, executá-lo, usar mensagens de erro ou resultados de testes como feedback para corrigir bugs.

- **Escrita e refinamento**: Gerar um rascunho, refletir sobre sua clareza e tom, e depois revisá-lo.

- **Resolução de problemas complexos**: Gerar um plano, avaliar sua viabilidade e refiná-lo com base na avaliação.

- **Recuperação de informações**: Pesquisar informações e usar um LLM avaliador para verificar se todos os detalhes necessários foram encontrados antes de apresentar a resposta.

```python

import os
import json
from google import genai
from pydantic import BaseModel
import enum
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
class EvaluationStatus(enum.Enum):
    PASS = "PASSOU"
    FAIL = "FALHOU"
 
class Evaluation(BaseModel):
    evaluation: EvaluationStatus
    feedback: str
    reasoning: str
 
# --- Geração da Função Inicial ---
def generate_poem(topic: str, feedback: str = None) -> str:
    prompt = f"Escreve um pequeno poema de 4 linhas sobre {topic}."
    if feedback:
        prompt += f"\nIncorpore esse feedback: {feedback}"
    
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt
    )
    poem = response.text.strip()
    print(f"Poema gerado::\n{poem}")
    return poem
 
# --- Avaliação da Função ---
def evaluate(poem: str) -> Evaluation:
    print("\n--- Avaliação do poema ---")
    prompt_critique = f"""Critique o seguinte poema. O rima está boa? Ele tem exatemente 4 linhas? Ele é criativo? Responda com  PASSOU ou FALHOU e forneça um feedback.
 
Poema:
{poem}
"""
    response_critique = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt_critique,
        config={
            'response_mime_type': 'application/json',
            'response_schema': Evaluation,
        },
    )
    critique = response_critique.parsed
    print(f"Avaliação Status:{critique.evaluation}")
    print(f"Avaliação Feedback: {critique.feedback}")
    return critique
 
# Loop reflexivo 
max_iterations = 3
current_iteration = 0
topic = "Um robô aprendendo a pintar"
 
# Poema simulado que não vai passar na avaliação 
current_poem = "With circuits humming, cold and bright,\nA metal hand now holds a brush"
 
while current_iteration < max_iterations:
    current_iteration += 1
    print(f"\n--- Iteração {current_iteration} ---")
    evaluation_result = evaluate(current_poem)
 
    if evaluation_result.evaluation == EvaluationStatus.PASS:
        print("\nPoema final:")
        print(current_poem)
        break
    else:
        current_poem = generate_poem(topic, feedback=evaluation_result.feedback)
        if current_iteration == max_iterations:
            print("\nMáximo de iterações alcançadas. Ùltima chance:")
            print(current_poem)

```

### Padrão de Uso de Ferramentas

![ferramentas]({{ site.baseurl }}/assets/img/post_07_08/tool-use.png)

A *LLM* tem a capacidade de invocar funções ou APIs externas para interagir com o mundo exterior, recuperar informações ou realizar ações. Este padrão, frequentemente referido como Chamada de Função, é o mais amplamente reconhecido. A *LLM* recebe definições (nome, descrição, esquema de entrada) das ferramentas disponíveis (funções, APIs, bancos de dados, etc.). Com base na consulta do usuário, a *LLM* podem decidir chamar uma ou mais ferramentas, gerando uma saída estruturada (como JSON) que corresponde ao esquema exigido. Essa saída é usada para executar a ferramenta/função externa real, e o resultado é retornado para a *LLM*. A *LLM* então usa esse resultado para formular sua resposta final ao usuário. Isso estende vastamente as capacidades do *LLM* além de seus dados de treinamento.

#### Casos de Uso

- **Agendamento de compromissos usando uma API de calendário.**

- **Recuperação de preços de ações em tempo real via uma API financeira.**

- **Busca em um banco de dados vetorial por documentos relevantes (RAG).**

- **Controle de dispositivos de casa inteligente.**

- **Execução de trechos de código.**

```python

import os
from google import genai
from google.genai import types
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Defina a declaração da função para o modelo
weather_function = {
    "name": "pegue_temperatura_atual",
    "description": "Pega a temperatura atual de um lugar fornecido.",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "Nome da cidade, por exemplo: Recife",
            },
        },
        "required": ["location"],
    },
}
 
# Função de "placeholder" para simular a chamada da API
def get_current_temperature(location: str) -> dict:
    return {"Temperatura": "15", "Unidade": "Celsius"}
 
# Crie o objeto de configuração como mostrado no exemplo do usuário
# Use "client.models.generate_content with model", "contents", and "config"
tools = types.Tool(function_declarations=[weather_function])
contents = ["Qual é a temperatura em Londres agora?"]
response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=contents,
    config = types.GenerateContentConfig(tools=[tools])
)
 
# Processe (Confira para a chamada da função)
response_part = response.candidates[0].content.parts[0]
if response_part.function_call:
    function_call = response_part.function_call
    print(f"Função a ser chamada: {function_call.name}")
    print(f"Argumentos: {dict(function_call.args)}")
 
    # Execute a função
    if function_call.name == "pegue_temperatura_atual":        
        # Chame a função atual
        api_result = get_current_temperature(*function_call.args)
        # Adicione a chamada da função e o resultado da execução da função para o "contents"
        follow_up_contents = [
            types.Part(function_call=function_call),
            types.Part.from_function_response(
                name="pegue_temperatura_atual",
                response=api_result
            )
        ]
        # Gere a resposta final
        response_final = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=contents + follow_up_contents,
            config=types.GenerateContentConfig(tools=[tools])
        )
        print(response_final.text)
    else:
        print(f"Error: Função requisitada desconhecida: {function_call.name}")
else:
    print("Nenhuma chamada de função encontrada na resposta.")
    print(response.text)

```

### Padrão: Planejamento (Orquestrador-Trabalhadores)

![planejamento]({{ site.baseurl }}/assets/img/post_07_08/planning.png)

Um *LLM* planejador central decompõe uma tarefa complexa em uma lista dinâmica de subtarefas, que são delegadas a agentes trabalhadores especializados (muitas vezes utilizando o **Padrão de Uso de Ferramentas**) para execução. Este padrão tenta resolver problemas complexos que exigem raciocínio em várias etapas, criando um ***Plano inicial***. Este plano é gerado dinamicamente com base na entrada do usuário. As subtarefas são então atribuídas a agentes "Trabalhadores" que as executam, potencialmente em paralelo se as dependências permitirem. Um *LLM* "Orquestrador" ou "Sintetizador" coleta os resultados dos trabalhadores, reflete se o objetivo geral foi alcançado e sintetiza a saída final ou potencialmente inicia uma etapa de replanejamento, se necessário. Isso reduz a carga cognitiva em qualquer chamada de *LLM* única, melhora a qualidade do raciocínio, minimiza erros e permite a adaptação dinâmica do fluxo de trabalho. 

> A principal diferença do **Roteamento** é que o **Planejador** gera um plano de várias etapas, em vez de selecionar uma única próxima etapa.

#### Casos de Uso

- **Tarefas complexas de desenvolvimento de software**: Dividir "construir uma funcionalidade" em subtarefas de planejamento, codificação, teste e documentação.

- **Pesquisa e geração de relatórios**: Planejar etapas como busca de literatura, extração de dados, análise e redação de relatórios.

- **Tarefas multimodais**: Planejar etapas que envolvam geração de imagens, análise de texto e integração de dados.

- **Executar solicitações complexas de usuários**: "Planejar uma viagem de 3 dias para Paris, reservar voos e um hotel dentro do meu orçamento."

```python

import os
from google import genai
from pydantic import BaseModel, Field
from typing import List
 
# Configure o "client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Defina o Plano do "Schema"
class Task(BaseModel):
    task_id: int
    description: str
    assigned_to: str = Field(description="Qual tipo de ""trabalhador"" pode lidar com isso? Pesquisador, Escritor, Desenvolvedor")
 
class Plan(BaseModel):
    goal: str
    steps: List[Task]
 
# Passo 1: Gere o Plano (Planner LLM)
user_goal = "s."
 
prompt_planner = f"""
Crie um plano de passo a passo para alcançar o objetivo atual.
Atribua cada passo para um tipo de trabalhador hipotético (Pesquisador, Escritor).
 
Objetivo: {user_goal}
"""
 
print(f"Objetivo: {user_goal}")
print("Gerando plano...")
 
# Use um modelo capaz de planejar e estruturar saías
response_plan = client.models.generate_content(
    model='gemini-2.5-pro-preview-03-25',
    contents=prompt_planner,
    config={
        'response_mime_type': 'application/json',
        'response_schema': Plan,
    },
)
 
# Passo 2: Execute o plano (Orchestrator/Workers) 
for step in response_plan.parsed.steps:
    print(f"Passo {step.task_id}: {step.description} (Atribuído para: {step.assigned_to})")

```

### Padrão: Multiagente

![multi-agente]({{ site.baseurl }}/assets/img/post_07_08/multi-agent.png)

*Abordagem Coordenador-Gerente*

![multi-agente2]({{ site.baseurl }}/assets/img/post_07_08/multi-agent-2.png)

*Abordagem de enxame*

Múltiplos agentes distintos, cada um com um papel, persona ou especialidade específica, colaboram para atingir um objetivo comum. Este padrão utiliza agentes autônomos ou semi-autônomos. Cada agente pode ter um papel único (por exemplo, Gerente de Projeto, Desenvolvedor, Testador, Crítico), conhecimento especializado ou acesso a ferramentas específicas. Eles interagem e colaboram, frequentemente coordenados por um agente "coordenador" ou "gerente" central (como o **Gerente de Projetos** no diagrama) ou usando **lógica de "handoff"**, onde um agente passa o controle para outro agente.

#### Casos de Uso

- **Simulação de debates ou sessões de brainstorming com diferentes personas de IA.**

- **Criação de software complexo envolvendo agentes para planejamento, codificação, teste e implantação.**

- **Execução de experimentos virtuais ou simulações com agentes representando diferentes atores.**

- **Processos de escrita colaborativa ou criação de conteúdo.**

> O exemplo abaixo é uma simplificação de como usar o **padrão Multi-Agente** com lógica de handoff e saída estruturada. Recomendo acessar [LangGraph Multi-Agent Swarm](https://github.com/langchain-ai/langgraph-swarm-py) ou [Crew AI](https://www.crewai.com/open-source) para implementações mais completas.

```python

from google import genai
from pydantic import BaseModel, Field
 
# Configure o "Client" (certifique-se que a  GEMINI_API_KEY está configurada no seu ambiente)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])

# Defina as saidas estruturadas dos "Schemas"
class Response(BaseModel):
    handoff: str = Field(default="", description="O nome/função do agente para entregar a. Agentes disponíveis: 'Agente de Restaurante, 'Agente de Hotel'")
    message: str = Field(description="A mensagem de resposta para o usuário ou contexto para o próximo agente")
 
# Função do Agente
def run_agent(agent_name: str, system_prompt: str, prompt: str) -> Response:
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt,
        config = {'system_instruction': f'Você é {agent_name}. {system_prompt}', 'response_mime_type': 'application/json', 'response_schema': Response}
    )
    return response.parsed
 
 
# Defina o Sistema de "Prompts" para os Agentes
hotel_system_prompt = "Você é um Agente de Reservas de Hotel. Você SÓ lida com reservas de hotel. Se o usuário perguntar sobre restaurantes, voos ou qualquer outra coisa, responda com uma mensagem curta de encaminhamento contendo a solicitação original e defina o campo 'handoff' como 'Agente de Restaurantes'. Caso contrário, lide com a solicitação de hotel e deixe 'handoff' vazio."
restaurant_system_prompt = "Você é um Agente de Reservas de Restaurantes. Você lida com recomendações e reservas de restaurantes com base na solicitação do usuário fornecida no prompt."
 
# Prompt: para ser sobre um restaurante
initial_prompt = "Você pode reservar uma mesa em um restaurante italiano para 2 pessoas hoje à noite?"
print(f"Requisição inicial do usuário: {initial_prompt}")
 
# Execute o primeiro agente (Agente de Hotel) para forçar um encaminhamento lógico
output = run_agent("Agente de Hotel", hotel_system_prompt, initial_prompt)
 
# Simule uma interação com usuário para alterar o prompt e o encaminhamento
if output.handoff == "Agente de Restaurante":
    print("Encaminhamento acionado: Hotel para Restaurante")
    output = run_agent("Restaurant Agent", restaurant_system_prompt, initial_prompt)
elif output.handoff == "Agente de Hotel":
    print("Encaminhamento acionado: Restaurante para Hotel")
    output = run_agent("Agente de Hotel", hotel_system_prompt, initial_prompt)
 
print(output.message)    

```

### Combinando e Customizando esses Padrões

É importante lembrar que esses padrões não são regras fixas, mas blocos de construção flexíveis. Sistemas agênticos do mundo real frequentemente combinam elementos de múltiplos padrões. Um **agente de Planejamento** pode usar **Ferramentas**, e seus trabalhadores podem empregar a **Reflexão**. Um sistema **Multiagente** pode usar **Roteamento** internamente para atribuição de tarefas.

A chave para o sucesso com qualquer aplicação de *LLM*, especialmente sistemas agentivos complexos, é a avaliação empírica. Defina métricas, meça o desempenho, identifique gargalos ou pontos de falha e itere no seu design. Resista à superengenharia.

### Reconhecimento

Este *post* foi criado com o auxílio de pesquisa aprofundada e manual, buscando inspiração e informações em diversas fontes excelentes, incluindo:

- [5 Agentic AI Design Patterns](https://blog.dailydoseofds.com/p/5-agentic-ai-design-patterns)
- [What are Agentic Workflows?](https://weaviate.io/blog/what-are-agentic-workflows)
- [Building effective agents](https://www.anthropic.com/engineering/building-effective-agents)
- [How Agents Can Improve LLM Performance](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/)
- [Agentic Design Patterns](https://medium.com/@bijit211987/agentic-design-patterns-cbd0aae2962f)
- [Agent Recipes](https://www.agentrecipes.com/)
- [LangGraph Agentic Concepts](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)
- [OpenAI Agents Python Examples](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)
- [Anthropic Cookbook](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)

### Créditos

##### Esta postagem é uma tradução autorizada do post *[Zero to One: Learning Agentic Patterns](https://www.philschmid.de/agentic-pattern)*, de autoria de Philipp Schmid, *AI Developer Experience* do Google.

---
