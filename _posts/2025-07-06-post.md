---
layout: post
image: assets/img/post_07_06/header_post_07_06.png
image_caption: Uma imagem ilustrativa do Gemini AI
title: Utilizando a API do Google Gemini
subtitle: Use o Modelo de IA do Google em Python
date: 2025-07-06
author: Vinicius B. Pessoa
co_author: Gabriel A. Belo
categories: [Inteligência Artificial, Deep Learning]
tags: [IA Generativa, Chatbots, Criação de Conteúdo, DALL-E]
---

O **Gemini** é um novo modelo de linguagem que veio competir com o **ChatGPT/GPT** da OpenAI. A versão Gemini 2.5 (e subversões) veio competir diretamente com o GPT 4.1 (e subversões).

Neste artigo, não vamos usar a [interface do Gemini para usuários em geral](https://gemini.google.com/). O que queremos, aqui, é mostrar como acessar o _Gemini_ por meio do código de um programa Python, usando uma **API** (Interface de Programação de Aplicações) própria do Gemini.

**_E o que podemos fazer assim?_**

Podemos criar programas que usam o Gemini adaptado ao contexto específico da sua aplicação. Por exemplo:

-   Para acessar a dados específicos do usuário, como documentos ou atividades agendadas.
-   Para criar resumos, classificar textos, ou fazer outro tipo de raciocínio em linguagem natural.
-   Para fazer o Gemini disparar código Python criado por você.

Enfim, você pode mesclar a capacidade de linguagem e raciocínio do Gemini com a capacidade de programação Python tradicional para criar programas inovadores!

Neste documento, você saberá como acessar e utilizar o **Google Gemini** para esse fim.

### Sumário

- [Configurações Iniciais](#1---configurações-iniciais)
- [Verificando Modelos Disponíveis](#2---verificando-modelos-disponíveis)
- [Usando apenas Texto](#3---usando-apenas-texto)
- [Fornecendo Imagens](#4---fornecendo-imagens)
- [Utilizando para Conversar(Chat)](#5---utilizando-para-conversar-chat)

### 1 - Configurações Iniciais

#### 1.1 - Pré-requisitos

1 - Python 3.9 ou superior.

2 - Possuir o módulo  `google-generativeai`  instalado. Para isso, basta rodar o comando abaixo na linha de comando:

```bash

pip install google-generativeai
```

> **Observação**: É recomendado que as bibliotecas instaladas nos projetos em Python, sejam feitas nos [ambientes virtuais venv](https://dev.to/franciscojdsjr/guia-completo-para-usar-o-virtual-environment-venv-no-python-57bo).


#### 1.2 - Obtendo uma Chave para a API

Para acessar os modelos Gemini, você vai precisar de uma chave privada. Para gerá-la, siga estes passos:

1 - Acesse o seguinte site: [Google API Key.](https://aistudio.google.com/app/apikey?hl=pt-br)

2 - Caso ainda não esteja logado, faça o login utilizando sua conta Google e aceite os termos de serviço.

3 - Clique no botão **Criar chave de API** para criar uma chave para um novo projeto.

4 - Nesse momento será criado um projeto no Google Cloud Console (**Gemini API**) e você poderá copiar sua chave de API.

5 - Caso você já possua uma chave e queria criar outra, clique em **Criar chave de API** e na caixa contendo a opção de **Criar uma chave de API em um projeto atual**, selecione o projeto e prossiga.

6 - Parabéns sua chave de API já foi gerada. Agora copie a chave e salve-a em algum lugar seguro.

> **Observação:** A chave da API é um elemento crítico para acessar os serviços fornecidos pela Google API. Essa chave funciona como uma _senha_ que autentica suas solicitações para usar os modelos Gemini. Por esse motivo, deve-se prestar muita atenção  para que a mesma não fique disponível publicamente.

#### 1.3 - Armazenando a Chave da API

Para garantir que a chave da API não fique disponível publicamente, é recomendado salvá-la em uma variável de ambiente `GOOGLE_API_KEY`.

Você pode fazer isso de uma maneira prática criando um arquivo  `.env`  no seu projeto e carregando-o por meio de código Python.

Para isso, siga estes passos

1.  Crie um arquivo chamado exatamente `".env"`
2.  Abra-o como arquivo de texto e escreva este conteúdo:

```

GOOGLE_API_KEY=<Sua chave da API>
```

3 - Lembrar de substituir o trecho  `<Sua chave da API>`  pela sua chave.

> **Atenção:** Nunca faça o _commit_ do arquivo  `.env`  para um repositório git público! É grande o risco de que algum _hacker_ a descubra e use.
> Lembre de adicionar esse arquivo no `.gitignore` para evitar seu commit.

#### 1.4 - Carregando a Chave em Python

Agora, usaremos a biblioteca  `python-dotenv`  para carregar as variáveis de ambiente salvas em arquivo  `.env`. 

```bash

pip install python-dotenv
```

```python
import google.generativeai as gemini_ai
import os # Biblioteca nativa, que vamos usar para acessar o arquivo .env
from dotenv import load_dotenv # Importe load_dotenv

load_dotenv()  # Carrega as variáveis de ambiente do arquivo .env

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
```

Agora, basta fazer a chamada abaixo e…

```python

gemini_ai.configure(api_key=GEMINI_API_KEY)
```

…sua chave está pronta para uso!

### 2 - Verificando modelos disponíveis

Primeiramente devemos verificar quais são todos os modelos disponíveis para utilização disponibilizados pelo  **Google Gemini**.

Para isso basta utilizar o seguinte comando para receber a lista dos modelos existentes.

```python

modelos = gemini_ai.list_models()
```

Agora a variável **modelos** contém uma lista contendo todos os modelos existentes.

Para listar apenas os modelos generativos, utilize um loop  `for`  para filtrar os dados em que  `model.supported_generation_methods`  seja igual a  `'generateContent'`  como demonstra o código a seguir.

```python

for model in modelos:
    if "generateContent" in model.supported_generation_methods:
        print(f"Nome do Modelo: {model.name}")
```

Principalmente podemos trabalhar com os modelos mais atualizados, que são estes:

-   **gemini-2.5-flash**  - O de melhor custo benefício.  **Será utilizado nesta publicação**
-   **gemini-2.5-pro**  - O melhor modelo (ou seja, que gera o melhor conteúdo).

> **Dica**: Antes de programar com os modelos, você pode testar vários deles usando uma interface amigável: o  [Google AI Studio](https://aistudio.google.com/)!

### 3 - Usando apenas Texto

Depois dessa longa preparação, vamos usar o modelo!

Este código instancia uma classe para acessar o modelo remotamente (lembrando que o modelo não é carregado localmente).

```py

model = gemini_ai.GenerativeModel('gemini-2.5-flash')
```

Agora, vamos usá-lo em dois exemplos

#### 3.1 - Gerando resumo

Netse caso vou solicitar ao modelo para fazer um resumo de um trecho da [documentação da função](https://docs.python.org/pt-br/3/library/statistics.html#statistics.median_high) `statistics.median_high()` de Python.

Agora, vamos receber nossa primeira resposta do modelo via código, assim:

```py

resposta = model.generate_content("Faça um resumo em uma linha desse texto:"
    + "statistics.median_high(data) Retorna a mediana superior de dados numéricos. Se data for vazio, "
    + "a exceção StatisticsError é levantada. data pode ser uma sequência ou um iterável. "
    + "A mediana superior sempre é um membro do conjunto de dados. Quando o número de elementos for ímpar, "
    + "o valor intermediário é retornado. "
    + "Se houver um número par de elementos, o maior entre os dois valores centrais é retornado.")

```

Agora, você pode investigar algumas informações da  `resposta`  recebida com este código:

```py

print(resposta.text) # Resposta textual
print(resposta.usage_metadata) # Informações como número de tokens usados
```

A saída deve ser algo como:

```py

'''
`statistics.median_high(data)` retorna a mediana superior dos dados numéricos, sendo o valor central (se ímpar) 
ou o maior dos dois valores centrais (se par), sempre um membro do conjunto de dados, 
e levanta um erro se o dado for vazio.
'''

prompt_token_count: 104
candidates_token_count: 59
total_token_count: 1208
```

A saída retornada pelo campo  `.usage_metadata`  indica quantos “tokens” foram usados do modelo. Os tokens são proporcionais à quantidade de palavras e são usados para definir o valor a ser cobrado pelo uso. (Mas vamos usá-lo dentro dos limites gratuitos aqui).

#### 3.2 - Criando receitas

Neste segundo exemplo, queremos dar uma ideia de como usar esse modelo dentro de uma aplicação. A ideia seria criar um  _app_  que recomenda receitas para o usuário baseando-se nos ingredientes possuídos pelo usuário.

O código abaixo vai ler os ingredientes digitados e montar um  **prompt**  (ou  _query_), ou seja, uma pergunta ou comando para o modelo.

```py

# Pegando o input do suario (simulando uma aplicação).
ingredientes = input("Liste os ingredientes que você possui? ")

# Montando a string completa de 'prompt' ou 'query' (pergunta)
prompt = f"Com os seguintes ingredientes: {ingredientes}, escreva uma receita completa que utilize esses e apenas esses ingredientes."
```

Agora, podemos enviar o  _prompt_  ao modelo, receber a resposta e imprimir o resultado assim:

```py

# Gera a resposta que estamos buscando
resposta = model.generate_content(prompt)

# Imprime a resposta textual
print(resposta.text) 

# Informações sobre quantidades de tokens usados
print(resposta.usage_metadata) 
```

Caso o usuário tenha digitado  *'ovos, farinha de trigo, leite, açúcar, sal, manteiga'*, a saída seria algo assim:

```markdown

Com os ingredientes fornecidos – ovos, farinha de trigo, leite, açúcar, sal e manteiga – a receita perfeita para utilizá-los e apenas eles são as deliciosas Crêpes (ou Panquecas Finas), uma base versátil para doces ou salgados (neste caso, por levar açúcar, são para uso doce).

---

## Crêpes Clássicas Caseiras

Estas crêpes são leves, finas e com uma doçura sutil, perfeitas para serem apreciadas com um simples polvilhar de açúcar ou recheios variados (embora a receita foque apenas na massa com os ingredientes dados).

**Rendimento:** Aproximadamente 12-15 crêpes
**Tempo de Preparo:** 15 minutos + 30 minutos de descanso
**Tempo de Cozedura:** 20-30 minutos

### Ingredientes:

*   2 ovos grandes
*   1 e 1/2 xícaras (aproximadamente 180g) de farinha de trigo
*   2 colheres de sopa de açúcar
*   1 pitada de sal
*   2 xícaras (aproximadamente 480ml) de leite integral
*   2 colheres de sopa de manteiga sem sal, derretida (mais um pouco para untar a frigideira)

### Equipamentos Necessários:

*   Tigela grande
*   Batedor de arame (fouet) ou liquidificador
*   Frigideira antiaderente (de preferência de 20-24 cm de diâmetro)
*   Espátula

### Modo de Preparo:

1.  **Prepare a Massa:**
    *   Numa tigela grande, adicione a farinha de trigo, o açúcar e o sal. Misture bem com o batedor de arame.
    *   Numa tigela separada, bata ligeiramente os ovos. Adicione o leite aos ovos e misture bem.
    *   Despeje a mistura de líquidos (ovos e leite) gradualmente sobre os ingredientes secos na tigela maior, mexendo constantemente com o batedor de arame para evitar a formação de grumos. Mexa até obter uma massa lisa e homogênea.
    *   Por último, adicione a manteiga derretida à massa e misture bem.

2.  **Descanse a Massa:**
    *   Cubra a tigela com um pano de prato limpo ou plástico filme e leve à geladeira por pelo menos 30 minutos (o ideal seria 1 hora). Este passo é crucial para permitir que a farinha hidrate completamente e para que o glúten relaxe, resultando em crêpes mais macias e menos elásticas.

3.  **Cozinhe as Crêpes:**
    *   Aqueça uma frigideira antiaderente em fogo médio.
    *   Unte levemente a frigideira com um pouco de manteiga (pode usar um guardanapo de papel para espalhar). Remova o excesso.
    *   Com a frigideira quente, despeje cerca de 1/4 a 1/3 de xícara da massa no centro da frigideira (a quantidade exata depende do tamanho da sua frigideira). Imediatamente, incline e gire a frigideira em movimentos circulares para que a massa se espalhe uniformemente, formando uma camada fina que cubra todo o fundo.
    *   Cozinhe por 1-2 minutos, ou até que as bordas comecem a dourar e se soltar da frigideira. A superfície da crêpe deve parecer seca.
    *   Com uma espátula fina, vire a crêpe e cozinhe o outro lado por mais 30 segundos a 1 minuto, até dourar levemente.
    *   Transfira a crêpe pronta para um prato e repita o processo com o restante da massa, untando a frigideira levemente entre cada crêpe, se necessário.

### Dicas:

*   **Consistência da Massa:** Se a massa parecer muito espessa após o descanso, adicione um pouco mais de leite, uma colher de sopa de cada vez, até atingir a consistência desejada (fina o suficiente para espalhar facilmente na frigideira).
*   **Primeira Crêpe:** A primeira crêpe geralmente serve para "ajustar" a temperatura da frigideira e a quantidade de massa. Não se preocupe se ela não sair perfeita!
*   **Armazenamento:** As crêpes prontas podem ser guardadas na geladeira por 2-3 dias, cobertas, ou congeladas entre folhas de papel manteiga por até 1 mês.

### Sugestão de Consumo:

Sirva as crêpes quentes, polvilhadas com um pouco mais de açúcar. A simplicidade realça o sabor delicado da massa feita com esses ingredientes básicos.
---

prompt_token_count: 36
candidates_token_count: 1103
total_token_count: 2596
```

Como você pode perceber, uma receita nova foi gerada pelo modelo!

### 4 - Fornecendo Imagens

Agora, vamos ver vamos utilizar o serviço passando uma imagem como entrada.

Inicialmente vamos instalar a biblioteca `Pillow`

```bash

pip install Pillow
```

Vamos usar o módulo  `PIL.Image`  da biblioteca *Pillow* para exibir uma imagem localizada em  `Imagens\animal.jpg`. Crie uma célula de código e escreva o código abaixo para carregar uma imagem em uma variável.

```py

import PIL.Image

img = PIL.Image.open(r'Imagens\animal.jpg')
```

Saída:

![Imagem de uma Capivara](https://github.com/ViniciusBPessoa/Repositorio_Academico_Multidisciplinar/blob/main/Extencao/Post_01/Imagens/animal.jpg?raw=true)

A partir desse momento já é possível enviar essa imagem como entrada para o modelo, e obter a resposta dele.

```py

resposta = model.generate_content(img)
print(resposta.text)
```

Saída:

```

This is a capybara.
```

Vamos agora ver como enviar  **uma imagem e um texto**  para alguma finalidade específica. Por exemplo, vamos pedir para o modelo simplesmente identificar o animal presente na imagem de forma mais direta.

Para isso, basta que enviemos uma lista  **`[A, B]`**  contendo:

A - Texto de entrada, nesse caso  `"Identifique o animal nessa imagem"`.

B - Variável contendo a imagem.

```py

resposta = model.generate_content(["Identifique o animal nessa imagem", img])
print(resposta.text)
```

Saída:

```

O animal nessa imagem é uma **Capivara**.
```

### 5 - Utilizando para Conversar (Chat)

Agora, veremos como usar no modo de conversa (_chat_), permitindo uma interação contínua com um usuário.

Para isso, vamos usar uma classe especializada que armazena o histórico de mensagens trocadas. Você pode instanciá-la assim:

```py
chat = model.start_chat(history=[])
```

Agora, você pode enviar uma mensagem e ver a resposta assim:

```py

resposta = chat.send_message("Escreva uma linha de uma historia sobre o cachorro bernardo")
print(resposta.text)
```

Como resultado pode haver respostas assim:
```py

'''
Aqui estão algumas opções, escolha a que mais gostar:

1.  Com um bocejo preguiçoso, Bernardo esticou-se na sua cama, mas seus ouvidos já captavam os sons de um dia que prometia ser interessante.
2.  A cauda de Bernardo, um turbilhão de alegria, já indicava que mais um dia de aventuras estava prestes a começar.
3.  Bernardo, o cachorro de pelo macio e olhos curiosos, farejava o vento em busca de novas histórias para contar.
4.  Mesmo adormecido, Bernardo parecia sonhar com a próxima aventura, com as patinhas tremendo levemente.
'''
```

Agora é possível ver o histórico da última mensagem usando o seguinte laço:

```py

for message in chat.history:
    print(f'role - {message.role}',end=": ")
    print({message.parts[0].text})
```

No terminal, você verá esta saída:

```py

'''
role - user: {'Escreva uma linha de uma historia sobre o cachorro bernardo.'}
role - model: {'Aqui estão algumas opções, escolha a que mais gostar:\n\n1.  Com um bocejo preguiçoso, Bernardo esticou-se na sua cama, mas seus ouvidos já captavam os sons de um dia que prometia ser interessante.\n2.  A cauda de Bernardo, um turbilhão de alegria, já indicava que mais um dia de aventuras estava prestes a começar.\n3.  Bernardo, o cachorro de pelo macio e olhos curiosos, farejava o vento em busca de novas histórias para contar.\n4.  Mesmo adormecido, Bernardo parecia sonhar com a próxima aventura, com as patinhas tremendo levemente.'}
'''
```

Cada item de  `chat.get_history()`  (acessado pela variável  `message`) tem:

-   o campo  `.role`  para indicar quem mandou a mensagem (“user” ou “model”),
-   o campo  `.parts`  para indicar cada parte da mensagem, que pode ser texto ou imagem (na maioria dos casos, tem apenas uma parte, contendo texto).


Vamos enviar outra mensagem e examinar a resposta:

```py

resposta = chat.send_message("Troque o nome de bernardo para severino")
print(resposta.text)
```

Saída:

```

Claro! Aqui estão as opções com o nome Severino:

1.  Com um bocejo preguiçoso, Severino esticou-se na sua cama, mas seus ouvidos já captavam os sons de um dia que prometia ser interessante.
2.  A cauda de Severino, um turbilhão de alegria, já indicava que mais um dia de aventuras estava prestes a começar.
3.  Severino, o cachorro de pelo macio e olhos curiosos, farejava o vento em busca de novas histórias para contar.
4.  Mesmo adormecido, Severino parecia sonhar com a próxima aventura, com as patinhas tremendo levemente.
```

A forma de acessar o modelo acima garante que haja uma continuidade no envio de mensagens para o modelo, permitindo que cada nova mensagem seja analisada no contexto da conversa anterior (como demonstrado pela troca do nome do cachorro).

Agora você deve ser capaz de usar os modelos  [Gemini](https://gemini.google.com/)  para criar projetos inovadores!

---
