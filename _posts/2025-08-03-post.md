---
layout: post
image: assets/img/post_08_03/header_post_08_03.png
image_caption: Uma imagem ilustrativa sobre Engenharia de Contexto
title: Engenharia de Contexto para Agentes
subtitle: Aprenda a fornecer os dados ideais para o seu Agente
date: 2025-08-03
author: Lance Martin
tradutor: Gabriel A. Belo
categories: [Agentes de IA]
tags: [Prompt Engeenering, Context Engeenering]
---

Agentes precisam de contexto para executar tarefas. A **Engenharia de Contexto** é a arte e a ciência de preencher a janela de contexto com apenas as informações certas em cada etapa da trajetória de um Agente. Nesta publicação, eu agrupo a engenharia de contexto em algumas estratégias comuns vistas em muitos agentes populares hoje.

![context_engenering]({{ site.baseurl }}/assets/img/post_08_03/context_eng_overview.png)

### Engenharia de Contexto

Como Andrej Karpathy coloca, LLMs são como um [novo tipo de sistema operacional](https://www.youtube.com/watch?v=LCEmiRjPEtQ&t=620s). O LLM é como a CPU e sua [janela de contexto](https://docs.anthropic.com/en/docs/build-with-claude/context-windows) é como a RAM, servindo como a memória de execução do modelo. Assim como a RAM, a janela de contexto do LLM tem [capacidade limitada](https://lilianweng.github.io/posts/2023-06-23-agent/) para lidar com várias fontes de contexto. E assim como um sistema operacional gerencia o que se encaixa na RAM de uma CPU, a "engenharia de contexto" desempenha um papel semelhante. [Karpathy resume isso bem](https://x.com/karpathy/status/1937902205765607626):

> [Engenharia de contexto é a] "...a delicada arte e ciência de preencher a janela de contexto com a informação exata para o próximo passo."

![context_types]({{ site.baseurl }}/assets/img/post_08_03/context_types.png)

Quais são os tipos de contexto que precisamos gerenciar ao construir aplicações com LLMs? **Engenharia de contexto** é um guarda-chuva que se aplica a diferentes tipos de contexto:

- Instruções – prompts, memórias, exemplos few-shot, descrições de ferramentas, etc.
- Conhecimento – fatos, memórias, etc.
- Ferramentas – feedback de chamadas de ferramentas

### Engenharia de Contexto para Agentes

Este ano, o interesse em [Agentes](https://www.anthropic.com/engineering/building-effective-agents) cresceu enormemente à medida que os LLMs (Grandes Modelos de Linguagem) aprimoram suas capacidades de [raciocínio](https://platform.openai.com/docs/guides/reasoning?api-mode=responses) e chamada de ferramentas. Agentes intercalam chamadas de LLMs e chamadas de ferramentas, muitas vezes para [tarefas de longa duração](https://blog.langchain.com/introducing-ambient-agents/).

![agent_flow]({{ site.baseurl }}/assets/img/post_08_03/agent_flow.png)

No entanto, tarefas de longa duração e o acúmulo de feedback de chamadas de ferramentas significam que os Agentes frequentemente utilizam um grande número de tokens. Isso pode causar vários problemas: [Exceder o tamanho da janela de contexto](https://cognition.ai/blog/kevin-32b), aumentar o custo/latência, ou diminuir o desempenho do Agente. [Drew Breunig delineou claramente](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html) uma série de maneiras específicas pelas quais um contexto mais longo pode causar problemas de desempenho, incluindo:

- Envenenamento de Contexto: Quando uma alucinação entra no contexto.
- Distração de Contexto: Quando o contexto sobrecarrega o treinamento.
- Confusão de Contexto: Quando contexto supérfluo influencia a resposta.
- Conflito de Contexto: Quando partes do contexto discordam.

Com isso em mente, a [Cognition](https://cognition.ai/blog/dont-build-multi-agents) ressaltou a importância da engenharia de contexto:

> "Engenharia de contexto"... é efetivamente a função #1 dos engenheiros que constroem Agentes de IA.

[Antropic](https://www.anthropic.com/engineering/built-multi-agent-research-system) também deixou claro:

> Agentes frequentemente se envolvem em conversas que se estendem por centenas de turnos, exigindo estratégias cuidadosas de gerenciamento de contexto.

Então, como as pessoas estão lidando com esse desafio hoje? Eu agrupo as abordagens em 4 categorias — *write, select, compress, e isolate* — e dou alguns exemplos de cada uma abaixo.

![context_engenering]({{ site.baseurl }}/assets/img/post_08_03/context_eng_overview.png)

### Sumário

- [Contexto de Escrita](#contexto-de-escrita-write-context)
- [Contexto de Seleção](#contexto-de-seleção-select-context)
- [Contexto de Compressão](#contexto-de-compressão-compressing-context)
- [Contexto de Isolamento](#contexto-de-isolamento-isolating-context)
- [Conclusão](#conclusão)

### Contexto de Escrita (*Write Context*)

Escrever o contexto significa salvar o contexto fora da janela de contexto para ajudar um Agente a executar uma tarefa.

#### Bloco de rascunho (*Scratchpads*)

Quando humanos resolvem tarefas, eles fazem anotações e memorizam coisas para tarefas futuras relacionadas. Agentes também estão adquirindo essas capacidades! A tomada de notas via ["bloco de rascunho"](https://www.anthropic.com/engineering/claude-think-tool) é uma abordagem para persistir informações enquanto um Agente está executando uma tarefa. A ideia central é salvar informações fora da janela de contexto para que estejam disponíveis para o Agente. Um pesquisador de [multiagentes da Anthropic](https://www.anthropic.com/engineering/built-multi-agent-research-system) ilustra um exemplo claro disso:

> O *LeadResearcher* começa pensando na abordagem e salvando seu plano na Memória para persistir o contexto, já que se a janela de contexto exceder 200.000 tokens, ela será truncada e é importante manter o plano.

Blocos de rascunho podem ser implementados de algumas maneiras diferentes. Eles podem ser uma [chamada de ferramenta](https://www.anthropic.com/engineering/claude-think-tool) que simplesmente [escreve em um arquivo](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem). Também pode ser apenas um campo em um [objeto de estado](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) de tempo de execução que persiste durante a sessão. Em ambos os casos, os blocos de rascunho permitem que os agentes salvem informações úteis para ajudá-los a realizar uma tarefa.

#### Memórias (*Memories*)

Blocos de rascunho ajudam os agentes a resolver uma tarefa dentro de uma sessão específica, mas, às vezes, os agentes se beneficiam ao lembrar de coisas em muitas sessões. A [Reflexão](https://arxiv.org/abs/2303.11366) introduziu a ideia de reflexão após cada turno do Agente e a reutilização dessas memórias autogeradas. Já os [Agentes Generativos](https://ar5iv.labs.arxiv.org/html/2304.03442) criaram memórias sintetizadas periodicamente a partir de coleções de feedback de agentes anteriores.

Esses conceitos foram incorporados em produtos populares como ChatGPT, Cursor e Windsurf, que possuem mecanismos para gerar automaticamente memórias de longo prazo com base nas interações entre usuário e Agente.

![context_memoring]({{ site.baseurl }}/assets/img/post_08_03/llm_write_memory.png)

### Contexto de Seleção (*Select Context*)

Selecionar o contexto significa puxar um contexto de fora para dentro de uma janela de contexto, ajudando um Agente a executar a tarefa.

#### Bloco de rascunho (*Scratchpads*)

O mecanismo para selecionar contexto de um bloco de rascunho depende de como ele é implementado. Se for uma ferramenta, um agente pode simplesmente lê-lo fazendo uma chamada de ferramenta. Se for parte do estado de tempo de execução do agente, o desenvolvedor pode escolher quais partes do estado expor ao agente em cada etapa. Isso proporciona um nível de controle refinado para expor o contexto do bloco de rascunho ao LLM em turnos posteriores.

#### Memórias (*Memories*)

Se os agentes têm a capacidade de salvar [memórias](https://arxiv.org/pdf/2309.02427), eles também precisam da capacidade de selecionar as memórias relevantes para a tarefa que estão executando. Isso pode ser útil por algumas razões. Os agentes podem selecionar exemplos few-shot ([memórias episódicas](https://langchain-ai.github.io/langgraph/concepts/memory/#memory-types)) para ver exemplos de comportamento desejado, instruções ([memórias procedurais](https://langchain-ai.github.io/langgraph/concepts/memory/#memory-types)) para direcionar o comportamento, ou fatos ([memórias semânticas](https://langchain-ai.github.io/langgraph/concepts/memory/#memory-types)) para dar ao agente um contexto relevante para a tarefa.

| Tipo de Memória | Armazenado | Exemplo Humano | Exemplo Agente
|-----|-----|-----|-----|
| Semântica | Fatos | Coisas aprendidas | Fatos sobre o usuário|
| Episódica | Experiências | Coisas feitas | Ações passadas |
| Procedural | Instruções | Instintos ou Reflexos | Prompt do Sistema do Agente  |

Um desafio é garantir que as memórias relevantes sejam selecionadas. Alguns agentes populares simplesmente usam um conjunto restrito de arquivos que são sempre puxados para o contexto. Por exemplo, muitos agentes de código usam arquivos para salvar instruções (memórias "procedurais") ou, em alguns casos, exemplos (memórias "episódicas"). O Claude Code usa o [`CLAUDE.md.`](https://docs.anthropic.com/en/docs/claude-code/overview) O Cursor e o Windsurf usam arquivos de regras.

No entanto, se um agente está armazenando uma [coleção](https://langchain-ai.github.io/langgraph/concepts/memory/#collection) maior de fatos e/ou relacionamentos (por exemplo, memórias semânticas), a seleção é mais difícil. O ChatGPT é um bom exemplo de um produto popular que armazena e seleciona a partir de uma grande coleção de memórias específicas do usuário.

Embeddings e/ou [grafos](https://neo4j.com/blog/developer/graphiti-knowledge-graph-memory/#:~:text=changes%20since%20updates%20can%20trigger,and%20holistic%20memory%20for%20agentic) de [conhecimento](https://arxiv.org/html/2501.13956v1#:~:text=In%20Zep%2C%20memory%20is%20powered,subgraph%2C%20and%20a%20community%20subgraph) para indexação de memória são comumente usados para auxiliar na seleção. Ainda assim, a seleção de memória é desafiadora. Na *AIEngineer World's Fair*, [Simon Willison](https://simonwillison.net/2025/Jun/6/six-months-in-llms/) compartilhou um exemplo de seleção de memória que deu errado: o ChatGPT buscou a localização dele nas memórias e a injetou inesperadamente em uma imagem solicitada. Esse tipo de recuperação de memória inesperada ou indesejada pode fazer com que alguns usuários sintam que a janela de contexto "não lhes pertence mais"!

#### Ferramentas (*Tools*)

Agentes utilizam ferramentas, mas podem ficar sobrecarregados se lhes forem fornecidas muitas delas. Isso geralmente ocorre porque as descrições das ferramentas podem se sobrepor, causando confusão no modelo sobre qual ferramenta usar. Uma abordagem é aplicar [*RAG* (geração aumentada por recuperação)](https://arxiv.org/abs/2410.14594) às descrições das ferramentas para buscar as ferramentas mais relevantes para uma tarefa com base na similaridade semântica. Alguns [artigos recentes](https://arxiv.org/abs/2505.03275) demonstraram que isso melhora a precisão na seleção de ferramentas em três vezes.

#### Conhecimento (*Knowledge*)

[*RAG*](https://github.com/langchain-ai/rag-from-scratch) (Retrieval Augmented Generation) é um tema vasto e pode ser um [desafio central](https://x.com/_mohansolo/status/1899630246862966837) na engenharia de contexto. Agentes de código são alguns dos melhores exemplos de RAG em produção em larga escala. Varun, da Windsurf, aborda bem alguns desses desafios:

> Indexar código != recuperação de contexto... [Estamos fazendo indexação e busca por embedding... [com] parsing de código AST e agrupamento ao longo de limites semanticamente significativos... a busca por embedding torna-se não confiável como heurística de recuperação à medida que o tamanho da base de código cresce... devemos confiar em uma combinação de técnicas como grep/busca de arquivo, recuperação baseada em grafo de conhecimento, e... uma etapa de reclassificação onde [o contexto] é classificado em ordem de relevância.

### Contexto de Compressão (*Compressing Context*)

Comprimir o contexto envolve reter apenas os tokens necessários para realizar uma tarefa.

#### Resumo do Contexto

Interações de agentes podem se estender por [centenas de turnos](https://www.anthropic.com/engineering/built-multi-agent-research-system) e usar chamadas de ferramentas com muitos tokens. O resumo é uma forma comum de lidar com esses desafios. Se você já usou o Claude Code, viu isso em ação. O Claude Code executa um ["auto-compact"](https://docs.anthropic.com/en/docs/claude-code/costs) depois que você excede 95% da janela de contexto e ele resume a [trajetória completa](https://langchain-ai.github.io/langgraph/concepts/memory/#manage-short-term-memory) das interações entre usuário e agente. Esse tipo de compressão ao longo de uma trajetória do agente pode usar várias estratégias, como resumo [recursivo](https://arxiv.org/pdf/2308.15022#:~:text=the%20retrieved%20utterances%20capture%20the,based%203) ou [hierárquico](https://alignment.anthropic.com/2025/summarization-for-monitoring/#:~:text=We%20addressed%20these%20issues%20by,of%20our%20computer%20use%20capability).

![context_curation]({{ site.baseurl }}/assets/img/post_08_03/context_curation.png)

Também pode ser útil [adicionar resumos](https://github.com/langchain-ai/open_deep_research/blob/e5a5160a398a3699857d00d8569cb7fd0ac48a4f/src/open_deep_research/utils.py#L1407) em pontos específicos do design de um agente. Por exemplo, pode ser usado para pós-processar certas chamadas de ferramentas (como ferramentas de busca que consomem muitos tokens). Como segundo exemplo, a [Cognition](https://cognition.ai/blog/dont-build-multi-agents) mencionou o resumo em fronteiras entre agentes para reduzir tokens durante a transferência de conhecimento. A sumarização pode ser um desafio se eventos ou decisões específicas precisarem ser capturados. A Cognition usa um modelo ajustado para isso, o que ressalta o trabalho que pode ser investido nesta etapa.

#### Corte de Contexto

Enquanto a sumarização geralmente usa um LLM para destilar as partes mais relevantes do contexto, o corte (trimming) frequentemente pode filtrar ou, como Drew Breunig aponta, ["podar"](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html) o contexto. Isso pode usar heurísticas codificadas (hard-coded), como remover [mensagens mais antigas](https://python.langchain.com/docs/how_to/trim_messages/) de uma lista de mensagens. Drew também menciona [Provence](https://arxiv.org/abs/2501.16214), um podador de contexto treinado para Perguntas e Respostas.

### Contexto de Isolamento (*Isolating Context*)

Isolar o contexto envolve dividí-lo para ajudar um agente a realizar uma tarefa.

#### Multiagente

Uma das formas mais populares de isolar o contexto é dividindo-o entre subagentes. Uma das motivações para a [biblioteca Swarm](https://github.com/openai/swarm) da OpenAI foi a ["separação de responsabilidades"](https://openai.github.io/openai-agents-python/ref/agent/), onde uma equipe de agentes pode lidar com subtarefas. Cada agente possui um conjunto específico de ferramentas, instruções e sua própria janela de contexto.

![context_multi_agent]({{ site.baseurl }}/assets/img/post_08_03/multi_agent.png)

O [pesquisador multiagente da Anthropic](https://www.anthropic.com/engineering/built-multi-agent-research-system) defende essa abordagem: muitos agentes com contextos isolados superaram um único agente, em grande parte porque a janela de contexto de cada subagente pode ser alocada para uma subtarefa mais restrita. Como o blog afirmou:

> [Subagentes operam] em paralelo com suas próprias janelas de contexto, explorando diferentes aspectos da questão simultaneamente.

Claro, os desafios com multiagentes incluem o uso de tokens (por exemplo, até 15 vezes mais tokens do que no chat, conforme relatado pela Anthropic), a necessidade de uma engenharia de prompt cuidadosa para planejar o trabalho dos subagentes e a coordenação dos subagentes.

#### Isolamento de Contexto com Ambientes

O [pesquisador da HuggingFace](https://huggingface.co/blog/open-deep-research#:~:text=From%20building%20,it%20can%20still%20use%20it) mostra outro exemplo interessante de isolamento de contexto. A maioria dos agentes usa [APIs de chamada de ferramentas](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview), que retornam objetos JSON (argumentos da ferramenta) que podem ser passados para as ferramentas (por exemplo, uma API de busca) para obter feedback (por exemplo, resultados de busca). A HuggingFace usa um [CodeAgent](https://huggingface.co/papers/2402.01030), que gera um código contendo as chamadas de ferramenta desejadas. O código então é executado em um [ambiente sandbox](https://e2b.dev/). O contexto selecionado (por exemplo, valores de retorno) das chamadas de ferramentas é então passado de volta para o LLM.

![context_isolation]({{ site.baseurl }}/assets/img/post_08_03/isolation.png)

Isso permite que o contexto seja isolado do LLM no ambiente. A Hugging Face observou que essa é uma ótima maneira de isolar objetos que consomem muitos tokens, em particular:

> [Code Agents permitem] um melhor manuseio do estado... Precisa armazenar esta imagem/áudio/outros para uso posterior? Sem problemas, basta atribuí-lo como uma variável em [seu estado e você [o usa depois]](https://deepwiki.com/search/i-am-wondering-if-state-that-i_0e153539-282a-437c-b2b0-d2d68e51b873).

#### Estado

Vale ressaltar que o [objeto de estado](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) em tempo de execução de um agente também pode ser uma excelente forma de isolar o contexto. Isso pode servir ao mesmo propósito que o isolamento (sandboxing). Um objeto de estado pode ser projetado com um [*schema*](https://langchain-ai.github.io/langgraph/concepts/low_level/#schema) (por exemplo, um [modelo Pydantic](https://docs.pydantic.dev/latest/concepts/models/)) que possui campos nos quais o contexto pode ser escrito. Um campo do *schema* (por exemplo, `messages`) pode ser exposto ao LLM em cada turno do agente, mas o esquema pode isolar informações em outros campos para uso mais seletivo.

### Conclusão

Os padrões para a engenharia de contexto de agentes ainda estão em evolução, mas podemos agrupar as abordagens comuns em quatro categorias: *write, select, compress, e isolate*.

- Escrever contexto significa salvá-lo fora da janela de contexto para ajudar um agente a realizar uma tarefa.
- Selecionar contexto significa puxá-lo para dentro da janela de contexto para ajudar um agente a realizar uma tarefa.
- Comprimir contexto envolve reter apenas os tokens necessários para realizar uma tarefa.
- Isolar contexto envolve dividi-lo para ajudar um agente a realizar uma tarefa.

Compreender e utilizar esses padrões é uma parte central da construção de agentes eficazes hoje em dia.

### Créditos

##### Esta postagem é uma tradução autorizada do post *[Context Engineering for Agents](https://rlancemartin.github.io/2025/06/23/context_engineering/)*, de autoria de [Lance Martin](https://x.com/RLanceMartin), *phd Stanford*.
